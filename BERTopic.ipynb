{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 主题建模\n",
    "\n",
    "围绕这些关键词（long COVID, Post COVID syndrome, long-haul COVID, post-acute sequelae of COVID），看看出现这些关键词的句子，如何描述长新冠。BERTopic主题建模，追踪主题随时间的演变，发现不同时期的热点话题 （每个topic15个关键词；尽量把不同国家topics的数量控制在一个范围，例如18-25个topics，尽量通过调整参数来实现，而不是把超出25的删掉）"
   ],
   "id": "e9e3a334cbc4ebfc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.用BERTopic对关键句子进行主题建模",
   "id": "8498cffa93068602"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T08:45:49.200642Z",
     "start_time": "2025-02-06T08:44:02.348586Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档名称： ./key words\\result_Australia 澳大利亚.csv\n",
      "文档长度： 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 220.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 5\n",
      "文档名称： ./key words\\result_Germany 德国.csv\n",
      "文档长度： 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 200.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 3\n",
      "文档名称： ./key words\\result_India 印度.csv\n",
      "文档长度： 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:10<00:00,  1.45s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 167.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 3\n",
      "文档名称： ./key words\\result_Singapore 新加坡.csv\n",
      "文档长度： 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 164.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 3\n",
      "文档名称： ./key words\\result_UK 英国.csv\n",
      "文档长度： 921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 29/29 [00:31<00:00,  1.07s/it]\n",
      "100%|██████████| 8/8 [00:00<00:00, 115.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 10\n",
      "文档名称： ./key words\\result_USA 美国.csv\n",
      "文档长度： 846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 27/27 [00:28<00:00,  1.04s/it]\n",
      "100%|██████████| 8/8 [00:00<00:00, 180.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 10\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from scipy.cluster import hierarchy as sch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from docx import Document\n",
    "\n",
    "# 加载本地的多语言嵌入模型\n",
    "local_model_path = \"./paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedding_model = SentenceTransformer(local_model_path)\n",
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "# 遍历文件夹下所有CSV文件\n",
    "for csv_file in glob.glob('./key words/*.csv'):\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "\n",
    "    df = df.dropna(subset=['key_sentences', 'load_date'])\n",
    "\n",
    "    print('文档名称：', csv_file)\n",
    "\n",
    "    print('文档长度：', len(df))\n",
    "\n",
    "    # 2. 确保时间格式的一致性\n",
    "    df['load_date'] = pd.to_datetime(df['load_date'])\n",
    "\n",
    "    # 3. 按年月重新组织数据\n",
    "    df['year_month'] = df['load_date'].dt.strftime('%Y-%m')\n",
    "    timestamps = df['year_month'].tolist()\n",
    "\n",
    "    texts = df['key_sentences'].tolist()\n",
    "\n",
    "    # 编码文本\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    pca_embeddings = rescale(PCA(n_components=5).fit_transform(embeddings))\n",
    "    # Start UMAP from PCA embeddings\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=5,\n",
    "        min_dist=0.0,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42,\n",
    "        init=pca_embeddings,\n",
    "    )\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words='english',ngram_range=(1,2))\n",
    " # Train the topic model using pre-trained sentence-transformers embeddings\n",
    "    topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model, top_n_words=15, nr_topics='auto')\n",
    "    topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    num_topics = len(topic_model.get_topics())\n",
    "    if num_topics > 10:\n",
    "        topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model, top_n_words=15, nr_topics=10)\n",
    "        topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    t = topic_model.get_topic_info()\n",
    "    # t.head(25)\n",
    "    doc = Document()\n",
    "\n",
    "    # Iterate over the DataFrame to get topic information\n",
    "    for index, row in t.iterrows():\n",
    "        topic_number = row['Topic']\n",
    "        frequency = row['Count']\n",
    "        keywords = [word for word, _ in topic_model.get_topic(topic_number)]\n",
    "        # Add a paragraph for each topic with its details\n",
    "        doc.add_paragraph(f\"Topic {topic_number}: {keywords}, Frequency: {frequency}\")\n",
    "\n",
    "\n",
    "    # 文档与主题图\n",
    "    # fig_documents = topic_model.visualize_documents(df['key_sentences'], embeddings=embeddings)\n",
    "    # 层次聚类图\n",
    "    linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "    hierarchical_topics = topic_model.hierarchical_topics(df['key_sentences'], linkage_function=linkage_function)\n",
    "    topic_labels = topic_model.generate_topic_labels(nr_words=15)\n",
    "    topic_model.set_topic_labels(topic_labels)\n",
    "    fig_hierarchy = topic_model.visualize_hierarchy (hierarchical_topics=hierarchical_topics, custom_labels=True)\n",
    "\n",
    "    # 时间序列图\n",
    "\n",
    "        # 1. 首先确保 topic_model 已经完成训练\n",
    "    print(\"主题数量:\", len(topic_model.get_topics()))\n",
    "\n",
    "    # 2. 使用更明确的参数调用\n",
    "    try:\n",
    "        topics_over_time = topic_model.topics_over_time(\n",
    "            docs=texts,\n",
    "            timestamps=timestamps,\n",
    "            nr_bins=9\n",
    "        )\n",
    "        topic_labels = topic_model.generate_topic_labels(nr_words=15)\n",
    "        topic_model.set_topic_labels(topic_labels)\n",
    "        fig_over_time = topic_model.visualize_topics_over_time(topics_over_time, custom_labels=True,width=1800,\n",
    "        height=600)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"错误类型:\", type(e))\n",
    "        print(\"错误信息:\", str(e))\n",
    "\n",
    "\n",
    "    # Extract the base name without the '.csv' extension\n",
    "    base_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    # Create a directory for this file's outputs\n",
    "    output_dir = os.path.join('./输出文件', base_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # When saving the HTML files, use the output_dir\n",
    "    # Save the document\n",
    "    doc.save(os.path.join(output_dir,'topics_and_frequencies.docx'))\n",
    "    # fig_documents.write_html(os.path.join(output_dir, 'documents_and_topics.html'))\n",
    "    fig_hierarchy.write_html(os.path.join(output_dir, 'hierarchy_clustering.html'))\n",
    "\n",
    "    fig_over_time.write_html(os.path.join(output_dir, 'over_time.html'))\n",
    "    try:\n",
    "        #主题分布图\n",
    "        fig_topics = topic_model.visualize_topics()\n",
    "        fig_topics.write_html(os.path.join(output_dir, 'intertopic_distance.html'))\n",
    "    except:\n",
    "        pass\n",
    "    model_dir = os.path.join(output_dir, 'model')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    topic_model.save(os.path.join(model_dir), serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "    # except:\n",
    "    #     print(csv_file)"
   ],
   "id": "783718d5ed85ca33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.用BERTopic对关键段落进行主题建模",
   "id": "d42677251b2924b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T08:49:19.239420Z",
     "start_time": "2025-02-06T08:47:11.116092Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档名称： ./key words\\result_Australia 澳大利亚.csv\n",
      "文档长度： 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:06<00:00,  1.32s/it]\n",
      "100%|██████████| 3/3 [00:00<00:00, 158.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 5\n",
      "成功生成时间序列图\n",
      "文档名称： ./key words\\result_Germany 德国.csv\n",
      "文档长度： 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:07<00:00,  1.52s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 231.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzjj\\anaconda3\\envs\\mentor\\lib\\site-packages\\scipy\\sparse\\linalg\\_eigen\\arpack\\arpack.py:1600: RuntimeWarning:\n",
      "\n",
      "k >= N for N * N square matrix. Attempting to use scipy.linalg.eigh instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功生成时间序列图\n",
      "文档名称： ./key words\\result_India 印度.csv\n",
      "文档长度： 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:11<00:00,  1.66s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 111.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 3\n",
      "成功生成时间序列图\n",
      "文档名称： ./key words\\result_Singapore 新加坡.csv\n",
      "文档长度： 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 143.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 3\n",
      "成功生成时间序列图\n",
      "文档名称： ./key words\\result_UK 英国.csv\n",
      "文档长度： 921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 29/29 [00:40<00:00,  1.39s/it]\n",
      "100%|██████████| 13/13 [00:00<00:00, 160.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 15\n",
      "成功生成时间序列图\n",
      "文档名称： ./key words\\result_USA 美国.csv\n",
      "文档长度： 846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 27/27 [00:35<00:00,  1.30s/it]\n",
      "100%|██████████| 9/9 [00:00<00:00, 164.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主题数量: 11\n",
      "成功生成时间序列图\n"
     ]
    }
   ],
   "execution_count": 10,
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from scipy.cluster import hierarchy as sch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "from docx import Document\n",
    "# 加载本地的多语言嵌入模型\n",
    "local_model_path = \"./paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "embedding_model = SentenceTransformer(local_model_path)\n",
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "# 遍历文件夹下所有CSV文件\n",
    "for csv_file in glob.glob('./key words/*.csv'):\n",
    "    df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "\n",
    "    df = df.dropna(subset=['key_paragraphs', 'load_date'])\n",
    "\n",
    "    print('文档名称：', csv_file)\n",
    "\n",
    "    print('文档长度：', len(df))\n",
    "\n",
    "    # 2. 确保时间格式的一致性\n",
    "    df['load_date'] = pd.to_datetime(df['load_date'])\n",
    "\n",
    "    # 3. 按年月重新组织数据\n",
    "    df['year_month'] = df['load_date'].dt.strftime('%Y-%m')\n",
    "    timestamps = df['year_month'].tolist()\n",
    "\n",
    "    texts = df['key_paragraphs'].tolist()\n",
    "\n",
    "    # 编码文本\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    pca_embeddings = rescale(PCA(n_components=5).fit_transform(embeddings))\n",
    "    # Start UMAP from PCA embeddings\n",
    "    umap_model = UMAP(\n",
    "        n_neighbors=15,\n",
    "        n_components=5,\n",
    "        min_dist=0.0,\n",
    "        metric=\"cosine\",\n",
    "        random_state=42,\n",
    "        init=pca_embeddings,\n",
    "    )\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words='english',ngram_range=(1,2))\n",
    " # Train the topic model using pre-trained sentence-transformers embeddings\n",
    "    topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model, top_n_words=15, nr_topics='auto')\n",
    "    topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    num_topics = len(topic_model.get_topics())\n",
    "    if num_topics > 15:\n",
    "        topic_model = BERTopic(umap_model=umap_model, vectorizer_model=vectorizer_model, top_n_words=15, nr_topics=15)\n",
    "        topics, _ = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "    t = topic_model.get_topic_info()\n",
    "    # t.head(25)\n",
    "    doc = Document()\n",
    "\n",
    "    # Iterate over the DataFrame to get topic information\n",
    "    for index, row in t.iterrows():\n",
    "        topic_number = row['Topic']\n",
    "        frequency = row['Count']\n",
    "        keywords = [word for word, _ in topic_model.get_topic(topic_number)]\n",
    "        # Add a paragraph for each topic with its details\n",
    "        doc.add_paragraph(f\"Topic {topic_number}: {keywords}, Frequency: {frequency}\")\n",
    "\n",
    "\n",
    "    # 文档与主题图\n",
    "    # fig_documents = topic_model.visualize_documents(df['key_sentences'], embeddings=embeddings)\n",
    "    # 层次聚类图\n",
    "    linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)\n",
    "    hierarchical_topics = topic_model.hierarchical_topics(df['key_paragraphs'], linkage_function=linkage_function)\n",
    "    topic_labels = topic_model.generate_topic_labels(nr_words=15)\n",
    "    topic_model.set_topic_labels(topic_labels)\n",
    "    fig_hierarchy = topic_model.visualize_hierarchy (hierarchical_topics=hierarchical_topics, custom_labels=True)\n",
    "\n",
    "    # 时间序列图\n",
    "\n",
    "        # 1. 首先确保 topic_model 已经完成训练\n",
    "    print(\"主题数量:\", len(topic_model.get_topics()))\n",
    "\n",
    "    # 2. 使用更明确的参数调用\n",
    "    try:\n",
    "        topics_over_time = topic_model.topics_over_time(\n",
    "            docs=texts,\n",
    "            timestamps=timestamps,\n",
    "            nr_bins=9\n",
    "        )\n",
    "        topic_labels = topic_model.generate_topic_labels(nr_words=15)\n",
    "        topic_model.set_topic_labels(topic_labels)\n",
    "        fig_over_time = topic_model.visualize_topics_over_time(topics_over_time, custom_labels=True, width=1800, height=600)\n",
    "        print(\"成功生成时间序列图\")\n",
    "    except Exception as e:\n",
    "        print(\"错误类型:\", type(e))\n",
    "        print(\"错误信息:\", str(e))\n",
    "\n",
    "\n",
    "    # Extract the base name without the '.csv' extension\n",
    "    base_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "    # Create a directory for this file's outputs\n",
    "    output_dir = os.path.join('./段落输出文件', base_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    # When saving the HTML files, use the output_dir\n",
    "    # Save the document\n",
    "    doc.save(os.path.join(output_dir,'topics_and_frequencies.docx'))\n",
    "    # fig_documents.write_html(os.path.join(output_dir, 'documents_and_topics.html'))\n",
    "    fig_hierarchy.write_html(os.path.join(output_dir, 'hierarchy_clustering.html'))\n",
    "\n",
    "    fig_over_time.write_html(os.path.join(output_dir, 'over_time.html'))\n",
    "    try:\n",
    "        #主题分布图\n",
    "        fig_topics = topic_model.visualize_topics()\n",
    "        fig_topics.write_html(os.path.join(output_dir, 'intertopic_distance.html'))\n",
    "    except:\n",
    "        pass\n",
    "    model_dir = os.path.join(output_dir, 'model')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    topic_model.save(os.path.join(model_dir), serialization=\"safetensors\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "    # except:\n",
    "    #     print(csv_file)"
   ],
   "id": "e56806ea225770e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "36feff5685c8dea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentor",
   "language": "python",
   "name": "mentor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
